# Local AI Chatbot

ä¸€å€‹ç°¡å–®çš„æœ¬åœ° AI èŠå¤©æ©Ÿå™¨äººï¼Œä½¿ç”¨ Python + Ollama + Open WebUI æ§‹å»ºã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸ¤– æœ¬åœ° AI æ¨¡å‹ (Llama 3.2 1B)
- ğŸ’¬ ç°¡æ½”çš„èŠå¤© API
- ğŸŒ Web UI ç•Œé¢ (Open WebUI)
- ğŸ³ Docker Compose ä¸€éµéƒ¨ç½²
- ğŸ“ å°è©±æ­·å²è¨˜éŒ„
- âš¡ GPU åŠ é€Ÿæ”¯æŒ

## æŠ€è¡“æ¶æ§‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Open WebUI    â”‚    â”‚  Chatbot API    â”‚    â”‚     Ollama      â”‚
â”‚   (Frontend)    â”‚â—„â”€â”€â–ºâ”‚   (FastAPI)     â”‚â—„â”€â”€â–ºâ”‚  (LLM Server)   â”‚
â”‚   Port: 3000    â”‚    â”‚   Port: 8000    â”‚    â”‚   Port: 11434   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## å¿«é€Ÿé–‹å§‹

### å‰ç½®éœ€æ±‚

- Docker å’Œ Docker Compose
- Git

### å®‰è£æ­¥é©Ÿ

1. **å…‹éš†å°ˆæ¡ˆ**
   ```bash
   git clone <repository-url>
   cd Local-Agentic-AI
   ```

2. **å•Ÿå‹•æœå‹™**
   ```bash
   docker-compose up -d
   ```

3. **ç­‰å¾…æ¨¡å‹ä¸‹è¼‰** (é¦–æ¬¡å•Ÿå‹•éœ€è¦æ™‚é–“)
   ```bash
   # æª¢æŸ¥æœå‹™ç‹€æ…‹
   docker-compose ps
   
   # ä¸‹è¼‰ Llama 3.2 1B æ¨¡å‹
   docker exec ollama ollama pull llama3.2:1b
   ```

4. **è¨ªå•æœå‹™**
   - èŠå¤© API: http://localhost:8000
   - Open WebUI: http://localhost:3000
   - Ollama æœå‹™: http://localhost:11434

## API ä½¿ç”¨æ–¹æ³•

### èŠå¤©ç«¯é»

```bash
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{"message": "Hello, how are you?"}'
```

**å›æ‡‰:**
```json
{
  "response": "I'm doing well, thank you for asking! How can I help you today?",
  "model": "llama3.2:1b"
}
```

### å¥åº·æª¢æŸ¥

```bash
curl http://localhost:8000/health
```

### åˆ—å‡ºå¯ç”¨æ¨¡å‹

```bash
curl http://localhost:8000/models
```

### æ¸…é™¤å°è©±æ­·å²

```bash
curl -X DELETE http://localhost:8000/chat/history
```

## é–‹ç™¼ç’°å¢ƒè¨­ç½®

å¦‚æœæ‚¨æƒ³è¦åœ¨æœ¬åœ°é–‹ç™¼ç’°å¢ƒä¸­é‹è¡Œï¼š

### ä½¿ç”¨ uv (æ¨è–¦)

1. **å®‰è£ uv**
   ```bash
   # Windows
   powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
   
   # macOS/Linux
   curl -LsSf https://astral.sh/uv/install.sh | sh
   ```

2. **å®‰è£ä¾è³´é …ç›®**
   ```bash
   uv sync
   ```

3. **é‹è¡Œæ‡‰ç”¨**
   ```bash
   # ç¢ºä¿ Ollama æ­£åœ¨é‹è¡Œ
   ollama serve
   
   # åœ¨å¦ä¸€å€‹çµ‚ç«¯
   uv run python app.py
   ```

### ä¾è³´é …ç›®

å°ˆæ¡ˆä½¿ç”¨ä»¥ä¸‹ä¸»è¦ä¾è³´é …ç›®ï¼š

- **ollama**: Ollama Python å®¢æˆ¶ç«¯
- **fastapi**: Web API æ¡†æ¶
- **uvicorn**: ASGI æœå‹™å™¨
- **pydantic**: è³‡æ–™é©—è­‰

å®Œæ•´ä¾è³´é …ç›®åˆ—è¡¨è«‹åƒè€ƒ `pyproject.toml`ã€‚

## é…ç½®é¸é …

å¯ä»¥é€šéç’°å¢ƒè®Šæ•¸è‡ªå®šç¾©é…ç½®ï¼š

```bash
# Ollama è¨­å®š
export OLLAMA_HOST=http://localhost:11434
export OLLAMA_MODEL=llama3.2:1b

# API è¨­å®š
export API_HOST=0.0.0.0
export API_PORT=8000

# Open WebUI è¨­å®š
export WEBUI_PORT=3000

# èŠå¤©è¨­å®š
export MAX_TOKENS=2048
export TEMPERATURE=0.7
```

## Docker æŒ‡ä»¤

```bash
# å•Ÿå‹•æ‰€æœ‰æœå‹™
docker-compose up -d

# åœæ­¢æ‰€æœ‰æœå‹™
docker-compose down

# æŸ¥çœ‹æ—¥èªŒ
docker-compose logs -f

# æŸ¥çœ‹ç‰¹å®šæœå‹™æ—¥èªŒ
docker-compose logs -f ollama
docker-compose logs -f chatbot-api
docker-compose logs -f open-webui

# é‡å»ºä¸¦å•Ÿå‹•
docker-compose up --build -d
```

## æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

1. **æ¨¡å‹ä¸‹è¼‰å¤±æ•—**
   ```bash
   # æ‰‹å‹•ä¸‹è¼‰æ¨¡å‹
   docker exec ollama ollama pull llama3.2:1b
   ```

2. **æœå‹™ç„¡æ³•å•Ÿå‹•**
   ```bash
   # æª¢æŸ¥æœå‹™ç‹€æ…‹
   docker-compose ps
   
   # æŸ¥çœ‹è©³ç´°æ—¥èªŒ
   docker-compose logs
   ```

3. **GPU æ”¯æŒ**
   å¦‚æœæ‚¨æœ‰ NVIDIA GPUï¼Œå¯ä»¥ä¿®æ”¹ `docker-compose.yml`ï¼š
   ```yaml
   ollama:
     image: ollama/ollama:latest
     deploy:
       resources:
         reservations:
           devices:
             - driver: nvidia
               count: 1
               capabilities: [gpu]
   ```

## å°ˆæ¡ˆçµæ§‹

```
Local-Agentic-AI/
â”œâ”€â”€ app.py                 # FastAPI ä¸»æ‡‰ç”¨
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py        # é…ç½®è¨­å®š
â”œâ”€â”€ docker-compose.yml     # Docker Compose é…ç½®
â”œâ”€â”€ Dockerfile            # Python æ‡‰ç”¨ Docker é¡åƒ
â”œâ”€â”€ init-model.sh         # æ¨¡å‹åˆå§‹åŒ–è…³æœ¬
â”œâ”€â”€ pyproject.toml        # uv å°ˆæ¡ˆé…ç½®
â”œâ”€â”€ uv.lock              # ä¾è³´é …ç›®é–å®šæ–‡ä»¶
â”œâ”€â”€ .gitignore           # Git å¿½ç•¥æ–‡ä»¶
â””â”€â”€ README.md            # å°ˆæ¡ˆèªªæ˜
```

## è²¢ç»

æ­¡è¿æäº¤ Issue å’Œ Pull Requestï¼

## æˆæ¬Š

æœ¬å°ˆæ¡ˆä½¿ç”¨ MIT æˆæ¬Šã€‚