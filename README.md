# Local AI Chatbot

ä½¿ç”¨ Python + Ollama + Open WebUI æ­å»ºçš„æœ¬åœ° AI èŠå¤©æ©Ÿå™¨äººç³»çµ±ã€‚

## ç³»çµ±æ¶æ§‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Open WebUI    â”‚    â”‚  FastAPI å¾Œç«¯   â”‚    â”‚     Ollama      â”‚
â”‚   (å‰ç«¯ä»‹é¢)     â”‚â—„â”€â”€â–ºâ”‚   (API æœå‹™)    â”‚â—„â”€â”€â–ºâ”‚   (LLM å¼•æ“)    â”‚
â”‚   Port: 3000    â”‚    â”‚   Port: 8000    â”‚    â”‚   Port: 11434   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## åŠŸèƒ½ç‰¹è‰²

- ğŸ¤– **æœ¬åœ°é‹è¡Œ**: ä½¿ç”¨ Llama 3.2 1B æ¨¡å‹ï¼Œè³‡æ–™ä¸å¤–æ´©
- ğŸ’¬ **ç¾ä»£ä»‹é¢**: Open WebUI æä¾›é¡ä¼¼ ChatGPT çš„ä½¿ç”¨é«”é©—  
- ğŸ”§ **API æœå‹™**: FastAPI å¾Œç«¯æä¾› RESTful API
- ğŸ³ **å®¹å™¨åŒ–**: Docker Compose ä¸€éµéƒ¨ç½²
- ğŸ“ **æœƒè©±è¨˜æ†¶**: æ”¯æ´å¤šè¼ªå°è©±
- âš¡ **GPU åŠ é€Ÿ**: è‡ªå‹•åµæ¸¬ä¸¦ä½¿ç”¨ NVIDIA GPU

## å¿«é€Ÿé–‹å§‹

### ç³»çµ±éœ€æ±‚

- Docker å’Œ Docker Compose
- 4GB+ è¨˜æ†¶é«” (æ¨è–¦ 8GB)
- (å¯é¸) NVIDIA GPU + Docker GPU æ”¯æ´

### å®‰è£æ­¥é©Ÿ

1. **ä¸‹è¼‰å°ˆæ¡ˆ**
   ```bash
   git clone https://github.com/Tony427/Local-Agentic-AI.git
   cd Local-Agentic-AI
   ```

2. **å•Ÿå‹•æœå‹™**
   ```bash
   docker-compose up -d
   ```

3. **ç­‰å¾…æ¨¡å‹ä¸‹è¼‰** (é¦–æ¬¡å•Ÿå‹•ç´„éœ€ 5-10 åˆ†é˜)
   ```bash
   # æŸ¥çœ‹ä¸‹è¼‰é€²åº¦
   docker-compose logs -f ollama
   
   # æ‰‹å‹•ä¸‹è¼‰æ¨¡å‹ (å¯é¸)
   docker exec ollama ollama pull llama3.2:1b
   ```

4. **é–‹å§‹ä½¿ç”¨**
   - èŠå¤©ä»‹é¢: http://localhost:3000
   - API æ–‡ä»¶: http://localhost:8000/docs
   - å¥åº·æª¢æŸ¥: http://localhost:8000/health

## è¨­å®šèˆ‡é…ç½®

### ç’°å¢ƒè®Šæ•¸è¨­å®š

å»ºç«‹ `.env` æª”æ¡ˆè‡ªè¨‚è¨­å®šï¼š

```bash
# Ollama è¨­å®š
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2:1b

# API è¨­å®š
API_HOST=0.0.0.0
API_PORT=8000

# èŠå¤©åƒæ•¸
MAX_TOKENS=2048
TEMPERATURE=0.7
```

### GPU æ”¯æ´è¨­å®š

å¦‚æœä½ æœ‰ NVIDIA GPUï¼Œä¿®æ”¹ `docker-compose.yml` å•Ÿç”¨ GPU åŠ é€Ÿï¼š

```yaml
ollama:
  image: ollama/ollama:latest
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
```

## API ä½¿ç”¨èªªæ˜

### èŠå¤©ç«¯é»

```bash
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{"message": "ä½ å¥½ï¼Œè«‹ä»‹ç´¹ä¸€ä¸‹ä½ è‡ªå·±"}'
```

å›æ‡‰æ ¼å¼ï¼š
```json
{
  "response": "ä½ å¥½ï¼æˆ‘æ˜¯ä¸€å€‹AIåŠ©æ‰‹ï¼Œé‹è¡Œåœ¨ä½ çš„æœ¬åœ°ç’°å¢ƒä¸­...",
  "model": "llama3.2:1b"
}
```

### å…¶ä»–ç«¯é»

```bash
# å¥åº·æª¢æŸ¥
curl http://localhost:8000/health

# æŸ¥çœ‹å¯ç”¨æ¨¡å‹
curl http://localhost:8000/models

# æ¸…é™¤å°è©±è¨˜éŒ„
curl -X DELETE http://localhost:8000/chat/history

# æŸ¥çœ‹å°è©±è¨˜éŒ„
curl http://localhost:8000/chat/history
```

## æ¸¬è©¦æ­¥é©Ÿ

### 1. æœå‹™æª¢æŸ¥

```bash
# ç¢ºèªæ‰€æœ‰å®¹å™¨æ­£å¸¸é‹è¡Œ
docker-compose ps

# æª¢æŸ¥æœå‹™å¥åº·ç‹€æ…‹
curl http://localhost:8000/health
```

### 2. åŸºæœ¬åŠŸèƒ½æ¸¬è©¦

```bash
# æ¸¬è©¦èŠå¤©åŠŸèƒ½
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{"message": "1+1ç­‰æ–¼å¤šå°‘ï¼Ÿ"}'

# æ¸¬è©¦å¤šè¼ªå°è©±
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{"message": "è«‹è¨˜ä½æˆ‘å«å°æ˜"}'

curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{"message": "æˆ‘å«ä»€éº¼åå­—ï¼Ÿ"}'
```

### 3. Web ä»‹é¢æ¸¬è©¦

1. é–‹å•Ÿç€è¦½å™¨å‰å¾€ http://localhost:3000
2. å»ºç«‹å¸³æˆ¶æˆ–ç›´æ¥é–‹å§‹èŠå¤©
3. ç¢ºèªèƒ½æ­£å¸¸æ”¶ç™¼è¨Šæ¯
4. æ¸¬è©¦æª”æ¡ˆä¸Šå‚³åŠŸèƒ½ (å¦‚æœéœ€è¦)

## é–‹ç™¼ç’°å¢ƒè¨­å®š

å¦‚æœä½ æƒ³åœ¨æœ¬åœ°é–‹ç™¼ç’°å¢ƒé‹è¡Œï¼š

### ä½¿ç”¨ uv (æ¨è–¦)

```bash
# å®‰è£ uv
curl -LsSf https://astral.sh/uv/install.sh | sh

# å®‰è£ç›¸ä¾å¥—ä»¶
uv sync

# å•Ÿå‹• Ollama (å¦é–‹çµ‚ç«¯)
ollama serve

# åŸ·è¡Œ FastAPI æœå‹™
uv run python app.py
```

### æ ¸å¿ƒä¾è³´

- `ollama` - Ollama Python å®¢æˆ¶ç«¯
- `fastapi` - Web API æ¡†æ¶
- `uvicorn` - ASGI ä¼ºæœå™¨
- `pydantic` - è³‡æ–™é©—è­‰

å®Œæ•´ä¾è³´æ¸…å–®è¦‹ `pyproject.toml`ã€‚

## å¸¸ç”¨æ“ä½œ

### Docker æŒ‡ä»¤

```bash
# æŸ¥çœ‹å³æ™‚æ—¥èªŒ
docker-compose logs -f

# é‡æ–°å•Ÿå‹•æœå‹™
docker-compose restart

# åœæ­¢æ‰€æœ‰æœå‹™
docker-compose down

# é‡å»ºä¸¦å•Ÿå‹•
docker-compose up --build -d

# æ¸…ç†æœªä½¿ç”¨çš„æ˜ åƒ
docker system prune -f
```

### æ•…éšœæ’é™¤

#### æ¨¡å‹ä¸‹è¼‰å¤±æ•—
```bash
# æ‰‹å‹•ä¸‹è¼‰æ¨¡å‹
docker exec ollama ollama pull llama3.2:1b

# æª¢æŸ¥å¯ç”¨ç©ºé–“
df -h
```

#### è¨˜æ†¶é«”ä¸è¶³
```bash
# ä½¿ç”¨æ›´å°çš„æ¨¡å‹
docker exec ollama ollama pull llama3.2:1b

# æˆ–èª¿æ•´ Docker è¨˜æ†¶é«”é™åˆ¶
```

#### ç„¡æ³•é€£æ¥æœå‹™
```bash
# æª¢æŸ¥é€£æ¥åŸ æ˜¯å¦è¢«ä½”ç”¨
netstat -tulpn | grep :3000
netstat -tulpn | grep :8000
netstat -tulpn | grep :11434

# é‡æ–°å•Ÿå‹• Docker
sudo systemctl restart docker
```

#### GPU ç„¡æ³•ä½¿ç”¨
```bash
# æª¢æŸ¥ NVIDIA Docker å®‰è£
nvidia-smi
docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi
```

## å°ˆæ¡ˆçµæ§‹

```
Local-Agentic-AI/
â”œâ”€â”€ app.py                 # FastAPI ä¸»ç¨‹å¼
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py        # è¨­å®šæª”
â”œâ”€â”€ docker-compose.yml     # Docker ç·¨æ’è¨­å®š
â”œâ”€â”€ Dockerfile            # Python å®¹å™¨å»ºç½®
â”œâ”€â”€ init-model.sh         # æ¨¡å‹åˆå§‹åŒ–è…³æœ¬
â”œâ”€â”€ pyproject.toml        # Python å°ˆæ¡ˆè¨­å®š
â”œâ”€â”€ uv.lock              # ä¾è³´é–å®šæª”
â””â”€â”€ README.md            # å°ˆæ¡ˆèªªæ˜
```

## é€²éšè¨­å®š

### æ•ˆèƒ½èª¿å„ª

1. **æ¨¡å‹å¿«å–**ï¼šé¦–æ¬¡ä¸‹è¼‰å¾Œæ¨¡å‹æœƒå¿«å–åœ¨ Docker volume
2. **è¨˜æ†¶é«”ç®¡ç†**ï¼šå¯åœ¨ `docker-compose.yml` è¨­å®šè¨˜æ†¶é«”é™åˆ¶
3. **GPU æ•ˆèƒ½**ï¼šå•Ÿç”¨ GPU å¯å¤§å¹…æå‡å›æ‡‰é€Ÿåº¦

### å®‰å…¨è€ƒé‡

1. **ç¶²è·¯è¨­å®š**ï¼šé è¨­åƒ…ç›£è½ localhostï¼Œç”Ÿç”¢ç’°å¢ƒè«‹é…ç½®é˜²ç«ç‰†
2. **è³‡æ–™éš±ç§**ï¼šæ‰€æœ‰å°è©±å‡åœ¨æœ¬åœ°è™•ç†ï¼Œä¸æœƒå‚³é€åˆ°å¤–éƒ¨ä¼ºæœå™¨
3. **å­˜å–æ§åˆ¶**ï¼šOpen WebUI æ”¯æ´ä½¿ç”¨è€…ç®¡ç†å’Œæ¬Šé™æ§åˆ¶

## æŠ€è¡“æ”¯æ´

- ğŸ› **å›å ±å•é¡Œ**: [GitHub Issues](https://github.com/Tony427/Local-Agentic-AI/issues)
- ğŸ“š **Ollama æ–‡ä»¶**: https://ollama.ai/docs
- ğŸŒ **Open WebUI æ–‡ä»¶**: https://docs.openwebui.com

## æˆæ¬Šæ¢æ¬¾

MIT License - è©³è¦‹ LICENSE æª”æ¡ˆ

---

**æç¤º**: é¦–æ¬¡å•Ÿå‹•æ™‚æ¨¡å‹ä¸‹è¼‰éœ€è¦æ™‚é–“ï¼Œè«‹è€å¿ƒç­‰å€™ã€‚å¦‚æœ‰å•é¡Œæ­¡è¿æå‡º Issueï¼